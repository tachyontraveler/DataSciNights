{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the essential libraries.\n",
    "1. While importing keras, there will be a message about Keras' current backend. \n",
    "2. The choices are \n",
    "    1. Tensorflow (by Google), \n",
    "    2. Theano (by LISA Lab at Université de Montréal). Theano is currently not under active development\n",
    "    3. CNTK (by Microsoft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess\n",
    "1. Here, a publicly available benchmarking dataset is downloaded via keras. (Reference: https://keras.io/datasets/#boston-housing-price-regression-dataset)\n",
    "\n",
    "2. Then sklearn's Standard Scalar function is used to scale the data by removing the mean and restrict each feature to a unit variance. That means, the mean is subtracted from each value and then it is divided by the standard deviation (Reference: https://stackoverflow.com/a/40853967/2374160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess():\n",
    "    # Load Data\n",
    "    (xtrain, ytrain), (xtest, ytest) = keras.datasets.boston_housing.load_data()\n",
    "    \n",
    "    # Feature Scaling\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    xtrain = scaler.fit_transform(xtrain)\n",
    "    xtest  = scaler.transform(xtest)\n",
    "    \n",
    "    return (xtrain,ytrain,xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the ANN\n",
    "1. ANN is first initialized as an object from the class Keras.models \n",
    "2. Then Layers are added. Initial input layer is automatically added while we define the first hidden layer.\n",
    "3. Final layer is a single ouput neuron. linear activation is used since it is a regression scenario.\n",
    "4. Finally the model is compiled with the information of loss value to compute and track, and the optimizer algorithm to follow while finding the local minima of loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neuralnet():\n",
    "    # initialize the model by \n",
    "    nnmodel = keras.models.Sequential() \n",
    "    \n",
    "    nnmodel.add(keras.layers.Dense(64, activation='sigmoid', input_shape=(13,)))\n",
    "    nnmodel.add(keras.layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    nnmodel.compile(optimizer=keras.optimizers.Nadam(lr=0.001), loss='mae')\n",
    "    return nnmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "1. This function requires training data (xtrain,ytrain), features of test data (xtest), and the un-trained ANN model\n",
    "2. xtrain and ytrain are used to train the model (also termed as fitting). \n",
    "3. xtest is used to predict values for the test data. Noitice that the ytest - the original target values of test data - is not shown to the  ANN model while training.\n",
    "4. nnmodel is the compiled ANN model object from build_neuralnet function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(xtrain,ytrain,xtest,nnmodel):\n",
    "    \n",
    "    earlystop = keras.callbacks.EarlyStopping(patience=10,verbose=1)\n",
    "    chkpt_mdl = keras.callbacks.ModelCheckpoint('best_model.h5',save_best_only=True,verbose=1)\n",
    "    calbcks   = [earlystop,chkpt_mdl]\n",
    "    \n",
    "    nnhistory = nnmodel.fit(xtrain,ytrain,callbacks=calbcks,epochs=500,\n",
    "                    validation_split=0.2,batch_size=16,verbose=1)\n",
    "    \n",
    "    best_nn   = keras.models.load_model('best_model.h5')\n",
    "    ypredict  = best_nn.predict(xtest)\n",
    "    \n",
    "    return (ypredict,nnhistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Functions (But strongly recommended)\n",
    "1. To plot the original and predicted target values of the test data against each other so as to compare\n",
    "2. mae: mean absolute error, mse: mean squared error\n",
    "3. Training data statistics are plotted in plot_history function to see the model training history. It is recommended to do so, to check for overfitting if callbacks are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(ytest,ypredict):\n",
    "    plt.scatter(ytest,ypredict)\n",
    "    plt.xlabel('True Values [1000$]')\n",
    "    plt.ylabel('Predictions [1000$]')\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(plt.xlim())\n",
    "    plt.ylim(plt.ylim())\n",
    "    _ = plt.plot([-100, 100], [-100, 100])\n",
    "    plt.show()\n",
    "    \n",
    "    print \"correlation r2 score is: \"+str(sklearn.metrics.r2_score(ytest,ypredict))\n",
    "    print \"correlation mse value is: \"+str(sklearn.metrics.mean_squared_error(ytest,ypredict))\n",
    "    print \"correlation mae value is: \"+str(sklearn.metrics.mean_absolute_error(ytest,ypredict))\n",
    "    \n",
    "    \n",
    "def plot_history(history):\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [1000$]')\n",
    "  plt.plot(history.epoch, np.array(history.history['loss']),\n",
    "           label='Train Loss')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val loss')\n",
    "  plt.legend()\n",
    "  plt.ylim([0, 50])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now call everything!!\n",
    "1. random seed is used as a random number generator reference. It is only for the reproducibility of exact same result in subsequent runs since the ANN models assign initial function values (weights, biases) randomly.\n",
    "2. Keras graph is cleared to start with a fresh ANN graph, without any variables from previous runs. It is safe to run that line everytime especially while using python notebooks\n",
    "3. The final final lines are calling the functions those were defined above. They are self explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "keras.backend.clear_session()\n",
    "\n",
    "xtrain,ytrain,xtest,ytest = data_preprocess()\n",
    "nnmodel                   = build_neuralnet()\n",
    "ypredict, nnhistory       = train(xtrain,ytrain,xtest,nnmodel)\n",
    "\n",
    "plot_data(ytest,ypredict)\n",
    "plot_history(nnhistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes, Tips\n",
    "1. When in doubt, X it. (X = Google, Bing, Yahoo, ...) \n",
    "2. Read the keras, sklearn documentations for specific function/class details - They are well maintained\n",
    "3. If something is not available in documentations, most probably it will be there in Stackoverflow\n",
    "4. If something is not available in Stackoverflow, ask your question in Stackoverflow (didn't see that coming?)\n",
    "5. If no one answers your question in Stackoverflow, Google Search lists many more links\n",
    "6. ANN modeling has many parameters to tune including\n",
    "    1. Optimizer: type, learning rate, momentum, ..\n",
    "    2. Loss function: mae, mse, cross_entropy, ...\n",
    "    3. Layers: Number of layers, Neurons per layer, activation function, initializers, regularizers, ...\n",
    "    4. ...\n",
    "7. Data preprocessing is important. Focus on accumulating maximum amount of information, and Engineering features. \n",
    "    1. Principal Component Analysis\n",
    "    2. Normalizer, Scaler,\n",
    "    3. Model based feature selection\n",
    "    4. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TFlow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
